import os.path

from datasets import load_dataset
import re
import nltk
from fasttext_pybind import fasttext
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import fasttext

def preprocess_text(inputString, lowercase, lemma, cleanPunctuation, stopword):
    newText = inputString
    if lowercase:
        #Do Lowercasing
        newText = newText.lower()

    #Tokenize the input
    tokenizedString = []
    for line in newText.splitlines():
        #Stores each line from input string as a tokenized array form of the original
        tokenizedString.append(line.split())
    newText = tokenizedString

    #Stopword Removal
    if stopword:
        #Load external stopword list
        mystops = stopwords.words('english')
        #From class colab, apply stopwords and tokenize
        mystops = set(mystops)
        newText = [[tok for tok in row if tok not in mystops] for row in newText]

    #Lemmatizer
    if lemma:
        #Lemmatize with NLTK
        lemmatizer = WordNetLemmatizer()
        newText = [[lemmatizer.lemmatize(word) for word in row] for row in newText]

    if cleanPunctuation:
        #Generated by Chatgpt
        newText = [[re.sub(r'[^\w\s]', '', word) for word in row] for row in newText]

    #Row below generated by ChatGPT, removes empty string elements
    #TODO: See if the following row is needed with fasttext
    newText = [[element for element in row if element] for row in newText if row and any(row)]
    return newText

#If the data is not processed yet, process the data and store it.  Else, retrieve the processed data
processed_data_path = "processed_data.txt"
training_features = []
if not os.path.exists(processed_data_path):
#     print("Loading Pretrained features")
#     with open(processed_data_path, "r", encoding="utf-8") as file:
#         training_features = file.read()
# else:
    #Load the dataset
    print("Loading Dataset")
    from datasets import load_dataset

    dataset = load_dataset("wikipedia", "20220301.simple")
    text_feature = dataset["train"]["text"]

    # Preprocess the data
    preprocessed_feature = []
    for count, feature in enumerate(text_feature):
        if count % 1000 == 0:
            print(f"Preprocessing {count}")
        preprocessed_feature.extend(preprocess_text(feature, lowercase=True, lemma=True, cleanPunctuation=True, stopword=True))
    flat_feature = [" ".join(row) for row in preprocessed_feature if row]

    #Save data
    with open(processed_data_path, "w", encoding="utf-8") as f:
        for line in flat_feature:
            f.write(line + "\n")

    #Pass to variable with larger scope
    # training_features = flat_feature

#apply fastText
print("Training skip-gram")
skip_gram_model = fasttext.train_unsupervised(processed_data_path, "skipgram")
print("Training cbow")
cbow_model = fasttext.train_unsupervised(processed_data_path, "cbow")