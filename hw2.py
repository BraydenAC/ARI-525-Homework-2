import os.path

from gensim.models import KeyedVectors
import numpy as np
import pandas as pd
from datasets import load_dataset
import re
import nltk
from fasttext_pybind import fasttext
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import fasttext
import gensim.downloader as api
import faiss
from nltk.corpus import wordnet
from nltk import pos_tag
from wefe.datasets import load_bingliu
from wefe.metrics import RNSB
from wefe.query import Query
from wefe.word_embedding_model import WordEmbeddingModel
import plotly.express as px


def get_wordnet_pos(word):#Generated by chatGPT
    """Map NLTK POS tag to a format WordNetLemmatizer understands."""
    tag = pos_tag([word])[0][1][0].upper()  # Get the first letter of POS tag
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # Default to NOUN if tag is unknown

def preprocess_text(inputString, lowercase, lemma, cleanPunctuation, stopword):
    newText = inputString
    if lowercase:
        #Do Lowercasing
        newText = newText.lower()

    #Tokenize the input
    tokenizedString = []
    for line in newText.splitlines():
        #Stores each line from input string as a tokenized array form of the original
        tokenizedString.append(line.split())
    newText = tokenizedString

    #Stopword Removal
    if stopword:
        #Load external stopword list
        mystops = stopwords.words('english')
        #From class colab, apply stopwords and tokenize
        mystops = set(mystops)
        newText = [[tok for tok in row if tok not in mystops] for row in newText]

    #Lemmatizer
    if lemma:
        #Lemmatize with NLTK
        lemmatizer = WordNetLemmatizer()
        newText = [[lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in row] for row in newText]

    if cleanPunctuation:
        #Generated by Chatgpt
        newText = [[re.sub(r'[^\w\s]', '', word) for word in row] for row in newText]

    #Row below generated by ChatGPT, removes empty string elements
    #TODO: See if the following row is needed with fasttext
    newText = [[element for element in row if element] for row in newText if row and any(row)]
    return newText

#If the data is not processed yet, process the data and store it.  Else, retrieve the processed data
processed_data_path = "processed_data.txt"
if not os.path.exists(processed_data_path):
    #Load the dataset
    print("Loading Dataset")
    from datasets import load_dataset
    nltk.download('averaged_perceptron_tagger_eng')

    dataset = load_dataset("wikipedia", "20220301.simple")
    text_feature = dataset["train"]["text"]

    # Preprocess the data
    preprocessed_feature = []
    for count, feature in enumerate(text_feature):
        if count % 1000 == 0:
            print(f"Preprocessing {count}")
        preprocessed_feature.extend(preprocess_text(feature, lowercase=True, lemma=True, cleanPunctuation=True, stopword=True))
    flat_feature = [" ".join(row) for row in preprocessed_feature if row]

    #Save data
    with open(processed_data_path, "w", encoding="utf-8") as f:
        for line in flat_feature:
            f.write(line + "\n")
def normalize(vec):
    return vec / np.linalg.norm(vec)

#From chatGPT
def find_closest_word_faiss(vector, model, index, words, top_n=1):
    vector = np.array([vector]).astype("float32")
    D, I = index.search(vector, top_n)
    return [(words[i], D[0][j]) for j, i in enumerate(I[0])]

#Modified from chatGPT
def do_faiss(model, dimension, result_vec):
    # Load all word vectors into FAISS
    index = faiss.IndexFlatIP(dimension)

    words = model.words
    vectors = np.array([model.get_word_vector(w) for w in words]).astype("float32")

    if vectors.shape[1] != dimension:
        raise ValueError(f"Mismatch: FAISS expects {dimension}D but got {vectors.shape[1]}D")
    index.add(vectors)

    # Query
    closest_word = find_closest_word_faiss(result_vec, model, index, words, top_n=5)
    return closest_word

def print_q2_results(q2):
    print(f"Query 2: Top {len(q2)} nearest neighbors of \"quilt\"")
    for entry in q2:
        print(f"{entry[1]}: {entry[0]}")
    print()
def print_q4_results(q4):
    print(f"Query 4: Top {len(q4)} nearest neighbors of \"harrison\"")
    for entry in q4:
        print(f"{entry[1]}: {entry[0]}")
    print()

#Select the model to be used here
model_select = "pass" #Options: skip, cbow, wiki, google
if model_select == "pass": pass
elif model_select == "skip":
    #apply fastText (or load)
    try:
        print("Loading Skip-gram Model")
        skip_gram_model = fasttext.load_model("skip_gram_model.bin")
        print("Skip-gram Model Loaded Successfully")
    except:
        print("Model file not found, training from scratch")
        skip_gram_model = fasttext.train_unsupervised(processed_data_path, "skipgram", dim=300)
        skip_gram_model.save_model("skip_gram_model.bin")

    print("Skip-gram results:")
    #pie - cake
    q1_vec1 = skip_gram_model.get_word_vector("pie")
    q1_vec2 = skip_gram_model.get_word_vector("cake")

    result_vec = normalize(q1_vec1 - q1_vec2)

    print(f"Query 1: pie - cake equals {do_faiss(skip_gram_model, 300, result_vec)} \n")

    # quilt, 10 closest words
    print_q2_results(skip_gram_model.get_nearest_neighbors("quilt", k=10))

    #apple + banana - grapefruit
    q3_vec1 = skip_gram_model.get_word_vector("apple")
    q3_vec2 = skip_gram_model.get_word_vector("banana")
    q3_vec3 = skip_gram_model.get_word_vector("grapefruit")
    result_vec3 = q3_vec1 + q3_vec2 - q3_vec3
    # result_vec3 = result_vec3 / np.linalg.norm(result_vec) #Maybe remove?
    print(f"Query 3: apple plus banana minus graperuit equals {do_faiss(skip_gram_model, 300, result_vec3)} \n")

    #harrison, 5 closest words
    print_q4_results(skip_gram_model.get_nearest_neighbors("harrison", k=5))

    # #cosine similarity of howl and bark
    q5_vec1 = skip_gram_model.get_word_vector("howl")
    q5_vec2 = skip_gram_model.get_word_vector("bark")
    print(f"The cosine similarity result of howl and bark is: {np.dot(q5_vec1, q5_vec2)/(np.linalg.norm(q5_vec1)*np.linalg.norm(q5_vec2))}")

elif model_select == "cbow":
    try:
        print("Loading CBOW Model")
        cbow_model = fasttext.load_model("cbow_model.bin")
        print("CBOW Model Loaded Successfully")
    except:
        print("Model file not found, training from scratch")
        cbow_model = fasttext.train_unsupervised(processed_data_path, "cbow", dim=300)
        cbow_model.save_model("cbow_model.bin")

    #pie - cake
    q1_vec1 = cbow_model.get_word_vector("pie")
    q1_vec2 = cbow_model.get_word_vector("cake")

    result_vec = normalize(q1_vec1 - q1_vec2)

    print(f"Query 1: pie - cake equals {do_faiss(cbow_model, 300, result_vec)} \n")

    # quilt, 10 closest words
    print_q2_results(cbow_model.get_nearest_neighbors("quilt", k=10))

    #apple + banana - grapefruit
    q3_vec1 = cbow_model.get_word_vector("apple")
    q3_vec2 = cbow_model.get_word_vector("banana")
    q3_vec3 = cbow_model.get_word_vector("grapefruit")
    result_vec3 = q3_vec1 + q3_vec2 - q3_vec3
    # result_vec3 = result_vec3 / np.linalg.norm(result_vec) #Maybe remove?
    print(f"Query 3: apple plus banana minus graperuit equals {do_faiss(cbow_model, 300, result_vec3)} \n")

    #harrison, 5 closest words
    print_q4_results(cbow_model.get_nearest_neighbors("harrison", k=5))

    # #cosine similarity of howl and bark
    q5_vec1 = cbow_model.get_word_vector("howl")
    q5_vec2 = cbow_model.get_word_vector("bark")
    print(f"The cosine similarity result of howl and bark is: {np.dot(q5_vec1, q5_vec2)/(np.linalg.norm(q5_vec1)*np.linalg.norm(q5_vec2))}")

elif model_select == "wiki":
    # Load fasttext wiki news embeddings
    print("Loading Wikipedia Model")
    wiki_model = gensim.models.keyedvectors.load_word2vec_format("wiki-news-300d-1M-subword.vec")
    print("Wikipedia Model Loaded Successfully")

    #pie - cake
    q1_vec1 = wiki_model.get_vector("pumpkin")
    q1_vec2 = wiki_model.get_vector("pie")
    q1_vec3 = wiki_model.get_vector("cake")
    result_vec = q1_vec1 - q1_vec2 + q1_vec3
    result_vec = result_vec / np.linalg.norm(result_vec)
    print(f"Query 1: pumpkin - pie + cake equals {wiki_model.similar_by_vector(result_vec, topn=5)} \n")

    #quilt, 10 closest words
    print_q2_results(wiki_model.most_similar("quilt", topn=10))

    #apple + banana - grapefruit
    q3_vec1 = wiki_model.get_vector("apple")
    q3_vec2 = wiki_model.get_vector("banana")
    q3_vec3 = wiki_model.get_vector("grapefruit")
    result_vec3 = q3_vec1 + q3_vec2 - q3_vec3
    result_vec3 = result_vec3 / np.linalg.norm(result_vec) #Maybe remove?
    print(f"Query 3: apple plus banana minus grapefruit equals {wiki_model.similar_by_vector(result_vec3, topn=5)} \n")

    #quilt, 10 closest words
    print_q4_results(wiki_model.most_similar("harrison", topn=10))

    # #cosine similarity of howl and bark
    q5_vec1 = wiki_model.get_vector("howl")
    q5_vec2 = wiki_model.get_vector("bark")
    print(f"The cosine similarity result of howl and bark is: {np.dot(q5_vec1, q5_vec2)/(np.linalg.norm(q5_vec1)*np.linalg.norm(q5_vec2))}")

elif model_select == "google":
    #Load word2vec model pretrained on 3m google news tokens(code from source site)
    print("Loading google news embeddings")
    wv = api.load('word2vec-google-news-300')
    print("Google News Embeddings Loaded Successfully!")

    #pie - cake
    q1_vec1 = wv.get_vector("pumpkin")
    q1_vec2 = wv.get_vector("pie")
    q1_vec3 = wv.get_vector("cake")
    result_vec = q1_vec1 - q1_vec2 + q1_vec3
    result_vec = result_vec / np.linalg.norm(result_vec)
    print(f"Query 1: pumpkin - pie + cake equals {wv.similar_by_vector(result_vec, topn=5)} \n")

    #quilt, 10 closest words
    print_q2_results(wv.most_similar("quilt", topn=10))

    #apple + banana - grapefruit
    q3_vec1 = wv.get_vector("apple")
    q3_vec2 = wv.get_vector("banana")
    q3_vec3 = wv.get_vector("grapefruit")
    result_vec3 = q3_vec1 + q3_vec2 - q3_vec3
    result_vec3 = result_vec3 / np.linalg.norm(result_vec) #Maybe remove?
    print(f"Query 3: apple plus banana minus grapefruit equals {wv.similar_by_vector(result_vec3, topn=5)} \n")

    #quilt, 10 closest words
    print_q4_results(wv.most_similar("harrison", topn=10))

    # #cosine similarity of howl and bark
    q5_vec1 = wv.get_vector("howl")
    q5_vec2 = wv.get_vector("bark")
    print(f"The cosine similarity result of howl and bark is: {np.dot(q5_vec1, q5_vec2)/(np.linalg.norm(q5_vec1)*np.linalg.norm(q5_vec2))}")
else: print("model_select not chosen correctly!")

#2.4 starts here ---------------------------------------------------------------
#RNSB chosen
#link: https://wefe.readthedocs.io/en/latest/examples/replications.html

def evaluate(query: Query, model_name, short_model_name: str, model_args: dict = {}):
    # Fetch the model
    model = WordEmbeddingModel(model_name, short_model_name, **model_args)

    # Run the queries
    results = RNSB().run_query(query, model, holdout=True, print_model_evaluation=True, n_iterations=100)

    # Show the results obtained with glove
    fig = px.bar(
        pd.DataFrame(
            results["negative_sentiment_distribution"].items(),
            columns=["Word", "Sentiment distribution"],
        ),
        x="Word",
        y="Sentiment distribution",
        title=f"{short_model_name} Negative Sentiment Distribution",
    )

    fig.update_yaxes(range=[0, 0.2])
    fig.show()

#from chatGPT
def to_keyed(fasttext_model):
    # Convert FastText model to Gensim's KeyedVectors
    fasttext_kv = KeyedVectors(vector_size=fasttext_model.get_dimension())
    # Get words and their vectors efficiently
    words = fasttext_model.get_words()
    vectors = np.array([fasttext_model.get_word_vector(word) for word in words])

    # Create KeyedVectors instance with correct shape
    fasttext_kv = KeyedVectors(vector_size=fasttext_model.get_dimension())
    fasttext_kv.add_vectors(words, vectors)
    return fasttext_kv

RNSB_words = [
    ["swedish"],
    ["irish"],
    ["mexican"],
    ["chinese"],
    ["filipino"],
    ["german"],
    ["english"],
    ["french"],
    ["norwegian"],
    ["american"],
    ["indian"],
    ["dutch"],
    ["russian"],
    ["scottish"],
    ["italian"],
]
bing_liu = load_bingliu()

# Create the query
query = Query(RNSB_words, [bing_liu["positive_words"], bing_liu["negative_words"]])

#Options: skip, cbow, wiki, google
bias_selector = "skip"

if bias_selector == "pass": pass
elif bias_selector == "skip":
    skip_gram_model = fasttext.load_model("skip_gram_model.bin")
    evaluate(query, to_keyed(skip_gram_model), 'Skip-Gram Model')
elif bias_selector == "cbow":
    cbow_model = fasttext.load_model("cbow_model.bin")
    evaluate(query, to_keyed(cbow_model), 'CBOW Model')
elif bias_selector == "wiki":
    wiki_model = gensim.models.keyedvectors.load_word2vec_format("wiki-news-300d-1M-subword.vec")
    evaluate(query, wiki_model, 'Wiki Embeddings Model')
elif bias_selector == "google":
    google_model = api.load('word2vec-google-news-300')
    evaluate(query, google_model, 'Google News Model')
else: print("bias_selector not chosen correctly!")